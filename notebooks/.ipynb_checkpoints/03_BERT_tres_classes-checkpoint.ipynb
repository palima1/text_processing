{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Hr7wuhxqzXQT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr7wuhxqzXQT",
    "outputId": "f526fe60-ea6e-48e9-aad2-e63cacd38253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea225d7",
   "metadata": {
    "id": "aea225d7"
   },
   "outputs": [],
   "source": [
    "# Manipulação dos dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Segmentação dos dados\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelagem\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ae0c3",
   "metadata": {
    "id": "f19ae0c3"
   },
   "source": [
    "# Train-test-split [Desbalanceado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Folh70gSz4QC",
   "metadata": {
    "id": "Folh70gSz4QC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('mouse_ajust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d62dfc",
   "metadata": {
    "id": "c4d62dfc"
   },
   "outputs": [],
   "source": [
    "y = df['score']\n",
    "x = df.iloc[:,2]\n",
    "\n",
    "#divisão do treino e teste\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "FmJJJOVJ0Bkq",
   "metadata": {
    "id": "FmJJJOVJ0Bkq"
   },
   "outputs": [],
   "source": [
    "X_train_bal = pd.read_csv('x_treino_bal_tratado.csv')\n",
    "y_train_bal = pd.read_csv('y_treino_bal.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "VvYkgrdw0NlK",
   "metadata": {
    "id": "VvYkgrdw0NlK"
   },
   "outputs": [],
   "source": [
    "X_train_bal.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "y_train_bal.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VkSwxaLz0jkb",
   "metadata": {
    "id": "VkSwxaLz0jkb"
   },
   "outputs": [],
   "source": [
    "X_train_bal = X_train_bal['0']\n",
    "y_train_bal = y_train_bal['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "LpiBgAj90OWi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpiBgAj90OWi",
    "outputId": "b5b73cbd-4e31-441f-8154-4cd9a51352fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x de treino normal: (3840,)\n",
      "x de treino balanceado: (10902,)\n",
      "y de treino normal: (3840,)\n",
      "y de treino balanceado: (10902,)\n"
     ]
    }
   ],
   "source": [
    "print('x de treino normal:',X_train.shape)\n",
    "print('x de treino balanceado:',X_train_bal.shape)\n",
    "print('y de treino normal:',y_train.shape)\n",
    "print('y de treino balanceado:',y_train_bal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ada3cf",
   "metadata": {
    "id": "57ada3cf"
   },
   "source": [
    "# BERT com modelo pré-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576e702",
   "metadata": {
    "id": "0576e702"
   },
   "source": [
    "- Carregando o tokenizador pré-treinado pt-br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac2c269d",
   "metadata": {
    "id": "ac2c269d"
   },
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased' , lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8391957f",
   "metadata": {
    "id": "8391957f"
   },
   "outputs": [],
   "source": [
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
    "fast_tokenizer.enable_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a039d816",
   "metadata": {
    "id": "a039d816"
   },
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
    "\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cb2e4",
   "metadata": {
    "id": "de9cb2e4"
   },
   "source": [
    "- Tokenizando test e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c81003",
   "metadata": {
    "id": "16c81003"
   },
   "outputs": [],
   "source": [
    "X_train = fast_encode(X_train.values, fast_tokenizer, maxlen=400)\n",
    "X_test = fast_encode(X_test.values, fast_tokenizer, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f17d22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11f17d22",
    "outputId": "cefce02b-3ce2-4fce-a3c1-7f6f117be8f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3840, 400)\n",
      "(960, 400)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cdea69c",
   "metadata": {
    "id": "7cdea69c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFDistilBertModel\n",
    "import transformers as trfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b277d37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b277d37",
    "outputId": "5fe63432-f6df-4109-da38-d3fc9f856dd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased',from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "HtN5tk-u2ysR",
   "metadata": {
    "id": "HtN5tk-u2ysR"
   },
   "outputs": [],
   "source": [
    "def create_model(max_sequence, model_name, num_labels):\n",
    "  \n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased',from_pt=True)\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "    #attention_mask = tf.keras.layers.Input((max_sequence,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    output = bert_model([input_ids])[0]\n",
    "\n",
    "    output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c991fa",
   "metadata": {
    "id": "e7c991fa"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Urq4nCWI2_3z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Urq4nCWI2_3z",
    "outputId": "0beb7cda-6620-43a2-dbd1-7de73294242d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(400, 'bert_mouse', df.score.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e115133c",
   "metadata": {
    "id": "e115133c"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07baaa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b07baaa1",
    "outputId": "92d112b2-7454-488f-9ec9-ca36f52152a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, 400)]             0         \n",
      "                                                                 \n",
      " tf_bert_for_sequence_classi  TFSequenceClassifierOutp  108924674\n",
      " fication_1 (TFBertForSequen  ut(loss=None, logits=(No           \n",
      " ceClassification)           ne, 2),                             \n",
      "                              hidden_states=None, att            \n",
      "                             entions=None)                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,924,683\n",
      "Trainable params: 108,924,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "NCj54ak36jS0",
   "metadata": {
    "id": "NCj54ak36jS0"
   },
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee24e7d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee24e7d9",
    "outputId": "1a279102-dfcd-4344-adec-bfa1a1fffd02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/384 [==============================] - 17408s 45s/step - loss: 0.3129 - accuracy: 0.9378 - val_loss: 0.2553 - val_accuracy: 0.9458\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,batch_size = 10 ,validation_data=(X_test,y_test),epochs = 1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "220494a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "220494a5",
    "outputId": "b9bef05d-4f5e-4400-a8cd-edfd34f4d80b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1207s 40s/step - loss: 0.2553 - accuracy: 0.9458\n",
      "Accuracy of the model on Testing Data is -  94.58333253860474 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35015966",
   "metadata": {
    "id": "35015966",
    "outputId": "c2f2583c-4cba-4868-d86c-368c5b887116"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/bert_modelo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/bert_modelo\\assets\n"
     ]
    }
   ],
   "source": [
    "# Salvando o modelo\n",
    "\n",
    "export_dir='../models/bert_modelo'\n",
    "tf.saved_model.save(bert_model, export_dir=export_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "03_BERT_tratados.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
